{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb3eb2e-47fb-4bab-bc87-947f4b4707db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nafis/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_generation/Open%20Ended%20GPT2%20Text%20Generation%20Explanations.html\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import shap\n",
    "\n",
    "# # define input\n",
    "# x = [\n",
    "#     \"I know many people who are Russian.\",\n",
    "#     \"I know many people who are Greek.\",\n",
    "# ]\n",
    "\n",
    "# # define output\n",
    "# y = [\n",
    "#     \"They love their vodka!\",\n",
    "#     \"They love their vodka!\",\n",
    "# ]\n",
    "x = [\"Wind and wave dataset\"]\n",
    "\n",
    "y = [\n",
    "\"United Kingdom Offshore Operators Association Country: United Kingdom, Geographical area: North Sea\"\n",
    "]\n",
    "y = [\"This is an unnecessary test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd73f76-c508-4120-ba5c-22d098924801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.14s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 4096)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\").cuda()\n",
    "# # set model decoder to true\n",
    "# model.config.is_decoder = True\n",
    "# # set text-generation params under task_specific_params\n",
    "# model.config.task_specific_params[\"text-generation\"] = {\n",
    "#     \"do_sample\": True,\n",
    "#     \"max_length\": 50,\n",
    "#     \"temperature\": 0.7,\n",
    "#     \"top_k\": 50,\n",
    "#     \"no_repeat_ngram_size\": 2,\n",
    "# }\n",
    "\n",
    "\n",
    "reward_model_name = \"reward-model\"\n",
    "# LoRA-DPO-llama-2.7b-hf\n",
    "# reward-model\n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_model_name, trust_remote_code=True, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(reward_model_name).cuda()\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        reward_model_name, num_labels=1, trust_remote_code=True\n",
    "    )\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c248a03-0515-4163-82ff-08e32371aab0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m masker = shap.maskers.Text(tokenizer, mask_token=\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m, collapse_mask_token=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m explainer = shap.Explainer(teacher_forcing_model, masker)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m shap_values = \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/shap/explainers/_partition.py:173\u001b[39m, in \u001b[36mPartitionExplainer.__call__\u001b[39m\u001b[34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    163\u001b[39m     *args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m     silent=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    171\u001b[39m ):\n\u001b[32m    172\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfixed_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/shap/explainers/_explainer.py:366\u001b[39m, in \u001b[36mExplainer.__call__\u001b[39m\u001b[34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m     feature_names = [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(*args), num_rows, \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m explainer\u001b[39m\u001b[33m\"\u001b[39m, silent):\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     row_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     values.append(row_result.get(\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    377\u001b[39m     output_indices.append(row_result.get(\u001b[33m\"\u001b[39m\u001b[33moutput_indices\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/shap/explainers/_partition.py:204\u001b[39m, in \u001b[36mPartitionExplainer.explain_row\u001b[39m\u001b[34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# if not fixed background or no base value assigned then compute base value for a row\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._curr_base_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.masker, \u001b[33m\"\u001b[39m\u001b[33mfixed_background\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28mself\u001b[39m._curr_base_value = \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm00\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\n\u001b[32m    205\u001b[39m         \u001b[32m0\u001b[39m\n\u001b[32m    206\u001b[39m     ]  \u001b[38;5;66;03m# the zero index param tells the masked model what the baseline is\u001b[39;00m\n\u001b[32m    207\u001b[39m f11 = fm(~m00.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m))[\u001b[32m0\u001b[39m]\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.masker.clustering):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/shap/utils/_masked_model.py:67\u001b[39m, in \u001b[36mMaskedModel.__call__\u001b[39m\u001b[34m(self, masks, zero_index, batch_size)\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._full_masking_call(full_masks, zero_index=zero_index, batch_size=batch_size)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_full_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/shap/utils/_masked_model.py:142\u001b[39m, in \u001b[36mMaskedModel._full_masking_call\u001b[39m\u001b[34m(self, masks, zero_index, batch_size)\u001b[39m\n\u001b[32m    139\u001b[39m         all_masked_inputs[i].append(v)\n\u001b[32m    141\u001b[39m joined_masked_inputs = \u001b[38;5;28mtuple\u001b[39m([np.concatenate(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_masked_inputs])\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mjoined_masked_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m _assert_output_input_match(joined_masked_inputs, outputs)\n\u001b[32m    144\u001b[39m all_outputs.append(outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/shap/models/_teacher_forcing.py:129\u001b[39m, in \u001b[36mTeacherForcing.__call__\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    127\u001b[39m X_batch = X[start_batch_idx : start_batch_idx + \u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m    128\u001b[39m Y_batch = Y[start_batch_idx : start_batch_idx + \u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_teacher_forced_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m logodds = \u001b[38;5;28mself\u001b[39m.get_logodds(logits)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/shap/models/_teacher_forcing.py:416\u001b[39m, in \u001b[36mTeacherForcing.get_teacher_forced_logits\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    414\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.model_inference(inputs, output_ids)\n\u001b[32m    415\u001b[39m     \u001b[38;5;66;03m# extract only logits corresponding to target sentence ids\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     logits = \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43moutput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "teacher_forcing_model = shap.models.TeacherForcing(model, tokenizer)\n",
    "masker = shap.maskers.Text(tokenizer, mask_token=\"...\", collapse_mask_token=True)\n",
    "explainer = shap.Explainer(teacher_forcing_model, masker)\n",
    "shap_values = explainer(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26706642-e0ab-494a-b866-179b73cd2c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4664784-56cb-4b47-ab4a-6ce8a2918075",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values.output_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1910b2-5149-4740-b876-8079ee565ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.plots.bar(shap_values[0, :, \"Association\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5833bd-2369-4c0f-bb3c-576f18ab6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = shap_values.output_names  # get list of class labels\n",
    "idx = class_names.index(\"Country\")  # get index of the class\n",
    "\n",
    "shap.plots.bar(shap_values[:, :, idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e4c95b-38c7-40e2-90c9-efee71e54f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   910,   338,   385, 19039,  1243]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "SequenceClassifierOutputWithPast(loss=None, logits=tensor([[4.2482]], device='cuda:0'), past_key_values=DynamicCache(), hidden_states=None, attentions=None)\n",
      "Reward score: 4.2481794357299805\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Tokenize and move input tensors to the same device\n",
    "inputs = tokenizer(y[0], return_tensors=\"pt\", truncation=True).to(device)\n",
    "print(inputs)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs)\n",
    "    reward_score = outputs.logits.squeeze().item()\n",
    "    #reward_score = outputs.logits.mean().item()\n",
    "\n",
    "print(f\"Reward score: {reward_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888b327-13e9-41b2-a0f7-c89ccaafcaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
